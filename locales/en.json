{
    "features": "Features",
    "faq": "FAQ",
    "download": "Download",
    "hero.title1": "Your local",
    "hero.title2": "AI toolkit.",
    "hero.subtitle": "Run Llama, DeepSeek, Qwen, Gemma locally on your computer.",
    "hero.download": "⬇️ Download Fllama",
    "spotlight.title": "Local AI Client",
    "spotlight.desc": "Fllama combines the power of local LLMs with a beautiful, intuitive interface. Enjoy privacy, speed, and full control.",
    "spotlight.feature1.title": "Multiple Model Support",
    "spotlight.feature1.desc": "Run Llama, Mistral, Gemma, Qwen and more — all locally.",
    "spotlight.feature2.title": "Blazing Fast",
    "spotlight.feature2.desc": "Instant startup, lightweight, no bloat. Optimized for speed.",
    "spotlight.feature3.title": "Full Privacy",
    "spotlight.feature3.desc": "All data stays on your device. No cloud, no tracking, ever.",
    "highlights.1": "Open Source",
    "highlights.2": "Fast Updates",
    "highlights.3": "Native Desktop",
    "features.title": "Features",
    "features.1.title": "Cross-platform",
    "features.1.desc": "Works on Windows, macOS (Intel & Apple Silicon), and Linux.",
    "features.2.title": "Fast & Lightweight",
    "features.2.desc": "Instant startup, minimal resource usage, no bloat.",
    "features.3.title": "Ollama Integration",
    "features.3.desc": "Chat with any Ollama model locally, including Llama 3, Mistral, Gemma, Qwen, and more.",
    "faq.title": "Frequently Asked Questions",
    "faq.1.question": "Does Fllama collect any data?",
    "faq.1.answer": "No. All your data stays local on your machine. Fllama does not collect or send any data anywhere.",
    "faq.2.question": "Can I use Fllama at my company or organization?",
    "faq.2.answer": "Yes! Fllama is free for internal business and personal use. See the license for details.",
    "faq.3.question": "What are the minimum requirements?",
    "faq.3.answer": "Ollama requires. Fllama runs on Windows, macOS (Intel/Apple Silicon), and Linux.",
    "faq.4.question": "What models can I run?",
    "faq.4.answer": "Any model supported by Ollama, including Llama, Mistral, Gemma, Qwen, and more. See the <a href=\"https://ollama.com/library\" target=\"_blank\">Ollama Model Library</a>.",
    "faq.5.question": "Is Fllama open source?",
    "faq.5.answer": "Yes! The code is available on <a href=\"https://github.com/arduradiokot/fllama\" target=\"_blank\">GitHub</a> under the GPL v2.0 license.",
    "download.title": "Download Fllama",
    "download.subtitle": "Choose the version for your operating system and start working with local AI models today.",
    "download.windows.title": "Windows",
    "download.windows.desc": "Full support for Windows 10 and 11. Easy installation and automatic updates.",
    "download.macos.title": "macOS",
    "download.macos.desc": "Native support for macOS 11+ with optimization for Apple Silicon and Intel processors.",
    "download.linux.title": "Linux",
    "download.linux.desc": "Works on all major Linux distributions. Easy installation via CLI.",
    "download.version": "Version 1.0.0",
    "download.button": "Download",
    "getstarted.title": "Get Started with Fllama",
    "getstarted.button": "Download for your OS",
    "getstarted.note": "Fllama is free for home and work use &bull; <a href=\"LICENSE\" target=\"_blank\">terms</a>",
    "getstarted.terms": "terms",
    "footer.copyright": "2025 Fllama",
    "footer.github": "GitHub",
    "footer.license": "License: GPL v2.0"
}